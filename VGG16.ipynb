{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLyin+ezkHVblkEMBBjfaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandilath/ML/blob/main/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify images with MNIST hand written digit dataset  by using VGG16  and  add 5% gaussian noise with dataset and give me code for above stated Recall,Precision,Classification Report  and plot its loss and accuracy curve and confusion matrix "
      ],
      "metadata": {
        "id": "KsEdplhA0Qkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "WgFxX8pg0XYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "cLjfOxRc0TYa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "PTlP0v3A3ZEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "SBrrDTV30dt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "0oDS48mi0feq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add 5% Gaussian Noise"
      ],
      "metadata": {
        "id": "oRmZfXwC0iFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    validation_split=0.1,\n",
        "    data_format='channels_last', # specify data format\n",
        "    preprocessing_function=lambda x: x + np.random.normal(0, 0.05, x.shape)\n",
        ")\n",
        "\n",
        "# add an extra dimension to x_train\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "train_generator = datagen.flow(\n",
        "    x_train, y_train, batch_size=64, subset='training')\n",
        "\n",
        "val_generator = datagen.flow(\n",
        "    x_train, y_train, batch_size=64, subset='validation')\n"
      ],
      "metadata": {
        "id": "EaLSNaVP44-i"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Model"
      ],
      "metadata": {
        "id": "Nqzb88EZ0uiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2u8WiQC5vUL",
        "outputId": "3a7c1a6b-9525-40c9-eb1a-49bf2b3ac801"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_13 (Conv2D)          (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 4, 4, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 2, 2, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 1, 1, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,637,066\n",
            "Trainable params: 33,637,066\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Model"
      ],
      "metadata": {
        "id": "uWyC1Rdz1F7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4zpXniQ1HDg",
        "outputId": "f3d7e3b2-d6f2-42a3-be52-2ca122517a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "816/844 [============================>.] - ETA: 2:50 - loss: 2.3019 - accuracy: 0.1110"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "O0KPO9js1ON0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test_classes, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "BhdcM_2H1Q9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the Loss and Accuracy Curve"
      ],
      "metadata": {
        "id": "pCAXITlJ1UjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Training Loss')\n",
        "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "92aL6-xq1VhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VyYfxOC7IAMV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkv4qprQIAlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2dOM6kLIA4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Add Gaussian noise to the images\n",
        "noise_factor = 0.05\n",
        "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
        "X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
        "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
        "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
        "\n",
        "# Display a single noisy image\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(X_train_noisy[0], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Display a single noise-free image\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(X_train[0], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "X_train_noisy = X_train_noisy.astype('float32') / 255\n",
        "X_test_noisy = X_test_noisy.astype('float32') / 255\n",
        "\n",
        "# One-hot encode the class labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model on noise-free images\n",
        "history = model.fit(X_train.reshape(-1, 28, 28, 1), y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on noise-free test images\n",
        "score = model.evaluate(X_test.reshape(-1, 28, 28, 1), y_test, verbose=0)\n",
        "print(f\"Accuracy on noise-free test images: {score[1]*100:.2f}%\")\n",
        "\n",
        "# Train the model on noisy images\n",
        "history_noisy = model.fit(X_train_noisy.reshape(-1, 28, 28, 1), y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on noisy test images\n",
        "score_no\n"
      ],
      "metadata": {
        "id": "Yj6zYx1bIBOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the data to (num_samples, height, width, depth)\n",
        "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
        "X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
        "\n",
        "# Add 5% Gaussian noise to the training and test data\n",
        "noise_factor = 0.05\n",
        "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
        "X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
        "\n",
        "# Clip the noisy images to be between 0 and 1\n",
        "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
        "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
        "\n",
        "# Create a VGG16 model\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_noisy, y_train, epochs=50, batch_size=128, validation_data=(X_test_noisy, y_test))\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = np.argmax(model.predict(X_test_noisy), axis=-1)\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "X_A-VswHIo2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TBl5FV2tKVmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPLETE CODE with 5% NOISE"
      ],
      "metadata": {
        "id": "mtuOMF7QKWHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the necessary libraries and loading the MNIST dataset:"
      ],
      "metadata": {
        "id": "2hpg7S3XKfFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n"
      ],
      "metadata": {
        "id": "mU7S-CgmKgYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add 5% Gaussian noise to the images in the training and testing datasets"
      ],
      "metadata": {
        "id": "axH4mdflKoHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 5% Gaussian noise to the images\n",
        "noise_factor = 0.05\n",
        "train_images_noisy = train_images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_images.shape)\n",
        "test_images_noisy = test_images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_images.shape)\n",
        "\n",
        "# Clip the noisy images to ensure that pixel values are between 0 and 1\n",
        "train_images_noisy = np.clip(train_images_noisy, 0., 1.)\n",
        "test_images_noisy = np.clip(test_images_noisy, 0., 1.)\n"
      ],
      "metadata": {
        "id": "jtxyY1m6KqmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " a single image from the dataset with and without noise:"
      ],
      "metadata": {
        "id": "2_Vj1_uUKxNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a single clean image\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.title('Clean image')\n",
        "plt.show()\n",
        "\n",
        "# Plot the same image with added noise\n",
        "plt.figure()\n",
        "plt.imshow(train_images_noisy[0], cmap='gray')\n",
        "plt.title('Noisy image')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1cSWfhN-K2D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the VGG16 model and train it on both the clean and noisy datasets:"
      ],
      "metadata": {
        "id": "V3Co3m4RK-qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VGG16 model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model on clean images\n",
        "history_clean = model.fit(train_images.reshape(-1, 28, 28, 1), train_labels, epochs=20, validation_split=0.1)\n",
        "\n",
        "# Train the model on noisy images\n",
        "history_noisy = model.fit(train_images_noisy.reshape(-1, 28, 28, 1), train_labels, epochs=20, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "uKIcPduJLWjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate its performance using the test dataset:"
      ],
      "metadata": {
        "id": "p_hl6gaALb3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on clean images\n",
        "test_loss_clean, test_acc_clean = model.evaluate(test_images.reshape(-1, 28, 28, 1), test_labels, verbose=0)\n",
        "print(f'Clean images - Test loss: {test_loss_clean:.4f}, Test accuracy: {test_acc_clean:.4f}')\n",
        "\n",
        "# Evaluate the model on noisy images\n",
        "test_loss_noisy, test_acc_noisy = model.evaluate(test_images_noisy.reshape(-1, 28, 28, 1), test_labels, verbose=0)\n",
        "print(f'Noisy images - Test loss: {test_loss_noisy:.4f}, Test accuracy: {test_acc_noisy:.4f}')\n"
      ],
      "metadata": {
        "id": "pL97bO91Ld7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate the precision, recall, and confusion matrix using scikit-learn"
      ],
      "metadata": {
        "id": "lGipLvUbLk_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test dataset\n",
        "test_preds = np.argmax(model.predict(test_images_noisy.reshape(-1, 28, 28, 1)), axis=-1)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(test_labels, test_preds))\n",
        "print(confusion_matrix(test_labels, test_preds))\n"
      ],
      "metadata": {
        "id": "8QH5PCyxLnGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot the loss and accuracy curves for both the clean and noisy datasets "
      ],
      "metadata": {
        "id": "BBEYQTwOLs7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation loss for clean and noisy datasets\n",
        "plt.plot(history_clean.history['loss'], label='Clean train')\n",
        "plt.plot(history_clean.history['val_loss'], label='Clean val')\n",
        "plt.plot(history_noisy.history['loss'], label='Noisy train')\n",
        "plt.plot(history_noisy.history['val_loss'], label='Noisy val')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "iybDWLgsLv5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}