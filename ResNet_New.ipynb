{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBVdS+iasWBoTI9sd39nwQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandilath/ML/blob/main/ResNet_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Architecture with MNIST hand written digit dataset "
      ],
      "metadata": {
        "id": "QLmxsPgen7Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the images\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Flatten, Dense, Add\n",
        "from keras.models import Model\n",
        "\n",
        "# Define the ResNet block\n",
        "def resnet_block(input_data, filters):\n",
        "    x = Conv2D(filters, kernel_size=(3,3), strides=(1,1), padding='same')(input_data)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, input_data])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define the ResNet model\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = resnet_block(x, 32)\n",
        "x = Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = resnet_block(x, 64)\n",
        "x = Conv2D(128, kernel_size=(3,3), strides=(2,2), padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = resnet_block(x, 128)\n",
        "x = Flatten()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predictions from one-hot encoding to class labels\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "# Generate the confusion matrix\n",
        "print(confusion_matrix\n",
        "\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bIpX8YUgoBXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " loading the MNIST dataset:"
      ],
      "metadata": {
        "id": "BECWXiYIrNGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the MNIST dataset:\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#preprocess the data by normalizing the pixel values and converting the labels to one-hot encoding:\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#define the ResNet architecture. Here, we'll use a modified version of ResNet-18, with the last few layers removed to adapt it to the MNIST dataset:\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_bn_relu(x, filters, kernel_size, strides):\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(x, filters, strides):\n",
        "    shortcut = x\n",
        "    x = conv_bn_relu(x, filters, kernel_size=3, strides=strides)\n",
        "    x = conv_bn_relu(x, filters, kernel_size=3, strides=1)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = conv_bn_relu(inputs, filters=64, kernel_size=3, strides=1)\n",
        "x = residual_block(x, filters=64, strides=1)\n",
        "x = residual_block(x, filters=64, strides=1)\n",
        "x = residual_block(x, filters=64, strides=1)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#compile the model and train it using the training data\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=128, epochs=20, validation_data=(x_test.reshape(-1, 28, 28, 1), y_test))\n",
        "\n",
        "#evaluate the model's performance on the test set and generate the desired metrics:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(x_test.reshape(-1, 28, 28, 1))\n",
        "y_pred = tf.argmax(y_pred, axis=1)\n",
        "y_true = tf.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "#plot the loss and accuracy curves, we can use Matplotlib:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly5PlE-QrNb8",
        "outputId": "d1dae7d5-bb56-42c7-fd3a-31544a78fa87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "460/469 [============================>.] - ETA: 41s - loss: 0.3520 - accuracy: 0.9286"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0roxcqvpuH_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset and preprocess it\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the ResNet block\n",
        "def res_block(x, filters, downsample=False):\n",
        "    shortcut = x\n",
        "    stride = 1\n",
        "    if downsample:\n",
        "        stride = 2\n",
        "        shortcut = Conv2D(filters, 1, strides=stride)(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    x = Conv2D(filters, 3, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define the ResNet model\n",
        "inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
        "\n",
        "x = res_block(x, 64)\n",
        "x = res_block(x, 64)\n",
        "\n",
        "x = res_block(x, 128, downsample=True)\n",
        "x = res_block(x, 128)\n",
        "\n",
        "x = res_block(x, 256, downsample=True)\n",
        "x = res_block(x, 256)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train.reshape(-1,28,28,1), y_train,\n",
        "                    validation_data=(x_test.reshape(-1,28,28,1), y_test),\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test.reshape(-1,28,28,1))\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "0JZ7SrjfuId0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise 5%"
      ],
      "metadata": {
        "id": "7iqf1Rv-1i5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.util import random_noise\n",
        "\n",
        "# Load the MNIST dataset and preprocess it\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Add 5% Gaussian noise to the input data\n",
        "x_train_noisy = random_noise(x_train, mode='gaussian', mean=0, var=0.05)\n",
        "x_test_noisy = random_noise(x_test, mode='gaussian', mean=0, var=0.05)\n",
        "\n",
        "# Define the ResNet model\n",
        "inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
        "\n",
        "x = res_block(x, 64)\n",
        "x = res_block(x, 64)\n",
        "\n",
        "x = res_block(x, 128, downsample=True)\n",
        "x = res_block(x, 128)\n",
        "\n",
        "x = res_block(x, 256, downsample=True)\n",
        "x = res_block(x, 256)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_noisy.reshape(-1,28,28,1), y_train,\n",
        "                    validation_data=(x_test_noisy.reshape(-1,28,28,1), y_test),\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test_noisy.reshape(-1,28,28,1))\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "G9xlzYID1jdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add 5% Gaussian noise"
      ],
      "metadata": {
        "id": "Sm-Ed-BZ2bJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this modified code, we first add 5% Gaussian noise to the training set using the random.normal function from NumPy. The loc parameter specifies the mean of the Gaussian distribution, which we set to 0.0, and the scale parameter specifies the standard deviation, which we set to 1.0. The size parameter specifies the shape of the noise array, which is the same as the shape of the training set. We then clip the noisy pixel values to be between 0.0 and 1.0.\n",
        "\n",
        "We then preprocess the data as before and train the model using the noisy"
      ],
      "metadata": {
        "id": "3_w4lXGM2kOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset and preprocess it\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Add 5% Gaussian noise to the training set\n",
        "noise_factor = 0.05\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train_noisy = x_train_noisy / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the ResNet model\n",
        "inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
        "\n",
        "x = res_block(x, 64)\n",
        "x = res_block(x, 64)\n",
        "\n",
        "x = res_block(x, 128, downsample=True)\n",
        "x = res_block(x, 128)\n",
        "\n",
        "x = res_block(x, 256, downsample=True)\n",
        "x = res_block(x, 256)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with noisy data\n",
        "history = model.fit(x_train_noisy.reshape(-1,28,28,1), y_train,\n",
        "                    validation_data=(x_test.reshape(-1,28,28,1), y_test),\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test.reshape(-1,28,28,1))\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "LFLsl3c02bsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Noise 10% "
      ],
      "metadata": {
        "id": "zDdMAn8z6yuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Add, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset and preprocess it\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Generate noisy images\n",
        "x_train_noisy = x_train + 0.1 * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + 0.1 * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip the noisy images to the [0, 1] range\n",
        "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)\n",
        "\n",
        "# Define the ResNet block\n",
        "def res_block(x, filters, downsample=False):\n",
        "    shortcut = x\n",
        "    stride = 1\n",
        "    if downsample:\n",
        "        stride = 2\n",
        "        shortcut = Conv2D(filters, 1, strides=stride)(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    x = Conv2D(filters, 3, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define the ResNet model\n",
        "inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
        "\n",
        "x = res_block(x, 64)\n",
        "x = res_block(x, 64)\n",
        "\n",
        "x = res_block(x, 128, downsample=True)\n",
        "x = res_block(x, 128)\n",
        "\n",
        "x = res_block(x, 256, downsample=True)\n",
        "x = res_block(x, 256)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_noisy.reshape(-1,28,28,1), y_train,\n",
        "                    validation_data=(x_test_noisy.reshape(-1,28,28,1), y_test),\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "# Plot the loss and accuracy curves\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test_noisy.reshape(-1,28,28,1))\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DicIRwws63TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gaussian Noise 15%"
      ],
      "metadata": {
        "id": "r24Oqy1C7fbw"
      }
    }
  ]
}